{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 1998\n",
      "Completed: 1999\n",
      "Completed: 2000\n",
      "Completed: 2001\n",
      "Completed: 2002\n",
      "Completed: 2003\n",
      "Completed: 2004\n",
      "Completed: 2005\n",
      "Completed: 2006\n",
      "Completed: 2007\n",
      "Completed: 2008\n",
      "Completed: 2009\n",
      "Completed: 2010\n",
      "Completed: 2011\n",
      "Completed: 2012\n",
      "Completed: 2013\n",
      "Completed: 2014\n",
      "Completed: 2015\n",
      "Completed: 2016\n",
      "Completed: 2017\n",
      "Completed: 2018\n",
      "Completed: 2019\n"
     ]
    }
   ],
   "source": [
    "# In this section, we are gonna build a web crawler for ECB press conference transcripts\n",
    "# The original link is: https://www.ecb.europa.eu/press/html/index.en.html\n",
    "\n",
    "# Import the necessary modules\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# The first of this work is collect all necessary links\n",
    "# To this end, open a dictionary\n",
    "final_links = {}\n",
    "\n",
    "# The ECB disclosures are available from 1998 until now, so, we will scrape all of them.\n",
    "base_url = \"https://www.ecb.europa.eu/press/pressconf/\"\n",
    "for year in range(1998, 2020):    # from 1998 to 2019\n",
    "    path = str(year) + \"/html/index.en.html\"\n",
    "    # Get the content of this page under the html form\n",
    "    html = requests.get(base_url + path)\n",
    "    # Parsing it using the model BeautifulSoup\n",
    "    parsing_html = BeautifulSoup(html.content, \"html.parser\")\n",
    "    # Since in each year, many meetings are held, and each meeting transcript is located\n",
    "    # ... in a separate link, we are gonna take each link\n",
    "    links = parsing_html.find_all(\"div\", string = \"ENGLISH\")\n",
    "    link_base_url = \"https://www.ecb.europa.eu\"\n",
    "    if links != []:\n",
    "        transcript_link = [link_base_url + link.find(\"a\")[\"href\"] for link in links]\n",
    "        final_links[str(year)] = transcript_link\n",
    "    else:\n",
    "        l1 = []\n",
    "        for i in parsing_html.find_all(\"div\", {\"class\": \"ecb-langSelector\"})[1:]:\n",
    "            l1.append(i.find(\"span\", {\"class\": \"offeredLanguage\"}))\n",
    "        l2 = []\n",
    "        for i in l1:\n",
    "            l2.append(i.find(\"a\", {\"class\": \"arrow\"}))\n",
    "        transcript_link = [link_base_url + link.get(\"href\") for link in l2]\n",
    "        final_links[str(year)] = transcript_link\n",
    "    print(\"Completed: \" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: 1998\n",
      "Download completed: 1999\n",
      "Download completed: 2000\n",
      "Download completed: 2001\n",
      "Download completed: 2002\n",
      "Download completed: 2003\n",
      "Download completed: 2004\n",
      "Download completed: 2005\n",
      "Download completed: 2006\n",
      "Download completed: 2007\n",
      "Download completed: 2008\n",
      "Download completed: 2009\n",
      "Download completed: 2010\n",
      "Download completed: 2011\n",
      "Download completed: 2012\n",
      "Download completed: 2013\n",
      "Download completed: 2014\n",
      "Download completed: 2015\n",
      "Download completed: 2016\n",
      "Download completed: 2017\n",
      "Download completed: 2018\n",
      "Download completed: 2019\n"
     ]
    }
   ],
   "source": [
    "# Now, scrape them\n",
    "# Open a folder\n",
    "for year in final_links.keys():\n",
    "    if not os.path.exists(\"./ECB PC transcripts/\" + year):\n",
    "        os.makedirs(\"./ECB PC transcripts/\" + year)\n",
    "    for link in final_links[year]:\n",
    "        tail_year = year[2:]\n",
    "        p = re.compile(tail_year + \"[0-9][0-9][0-9][0-9]\")\n",
    "        name = p.search(link)\n",
    "        response = urllib.request.urlretrieve(str(link), name.group() + \".txt\")\n",
    "        cwd = os.getcwd()\n",
    "        try:\n",
    "            os.rename(cwd + \"/\" + name.group() + \".txt\", \"./ECB PC transcripts/\" + year + \"/\" + name.group() + \".txt\")\n",
    "        except FileExistsError:\n",
    "            os.rename(cwd + \"/\" + name.group() + \".txt\", \"./ECB PC transcripts/\" + year + \"/\" + name.group() + \"_2\" + \".txt\")\n",
    "    print(\"Download completed: \" + year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
